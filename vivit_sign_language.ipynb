{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VC-tl47QZByv"
      },
      "outputs": [],
      "source": [
        "#Import Kaggle dataset\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "os.environ[\"KAGGLE_KEY\"] = os.getenv('KAGGLE_KEY')\n",
        "os.environ[\"KAGGLE_USERNAME\"] = os.getenv('KAGGLE_USERNAME')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/grassknoted/asl-alphabet?dataset_version_number=1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.03G/1.03G [02:10<00:00, 8.46MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files:/home/mich02/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "asl_dataset = \"ayuraj/asl-dataset\"\n",
        "asl_alphabet = \"grassknoted/asl-alphabet\"\n",
        "path = kagglehub.dataset_download(asl_alphabet)\n",
        "print(f\"Path to dataset files:{path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUdxEaEFst4-",
        "outputId": "93be1465-d7a4-4f70-db59-99bb06d6d988"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 47.9 kB of archives.\n",
            "After this operation, 116 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tree amd64 2.0.2-1 [47.9 kB]\n",
            "Fetched 47.9 kB in 0s (121 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 125082 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_2.0.2-1_amd64.deb ...\n",
            "Unpacking tree (2.0.2-1) ...\n",
            "Setting up tree (2.0.2-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Dataset URL: https://www.kaggle.com/datasets/ayuraj/asl-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading asl-dataset.zip to /content\n",
            "  0% 0.00/56.9M [00:00<?, ?B/s]\n",
            "100% 56.9M/56.9M [00:00<00:00, 1.45GB/s]\n",
            "Dataset URL: https://www.kaggle.com/datasets/grassknoted/asl-alphabet\n",
            "License(s): GPL-2.0\n",
            "Downloading asl-alphabet.zip to /content\n",
            " 98% 1.01G/1.03G [00:07<00:00, 110MB/s]\n",
            "100% 1.03G/1.03G [00:07<00:00, 143MB/s]\n"
          ]
        }
      ],
      "source": [
        "# First dataset\n",
        "!apt-get install tree\n",
        "!kaggle datasets download -d ayuraj/asl-dataset\n",
        "! unzip -q \"/content/asl-dataset.zip\"\n",
        "\n",
        "!kaggle datasets download -d grassknoted/asl-alphabet\n",
        "! unzip -q \"/content/asl-alphabet.zip\"\n",
        "\n",
        "!mv asl_alphabet_train/asl_alphabet_train/* asl_alphabet_train/\n",
        "!rm -r asl_alphabet_train/asl_alphabet_train/\n",
        "!mv asl_alphabet_test/asl_alphabet_test/* asl_alphabet_test/\n",
        "!rm -r asl_alphabet_test/asl_alphabet_test/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Go9_rlFSPJLK"
      },
      "outputs": [],
      "source": [
        "# ===== Top-level worker helpers (picklable) =====\n",
        "import uuid, random, cv2, numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "def _init_opencv_worker():\n",
        "    try:\n",
        "        cv2.setNumThreads(1)\n",
        "        cv2.ocl.setUseOpenCL(False)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "def _grabcut_mask_np(img_bgr):\n",
        "    h, w = img_bgr.shape[:2]\n",
        "    mask = np.zeros((h, w), np.uint8)\n",
        "    bgd, fgd = np.zeros((1, 65), np.float64), np.zeros((1, 65), np.float64)\n",
        "    pad = max(6, min(h, w) // 20)\n",
        "    rect = (pad, pad, w - 2*pad, h - 2*pad)\n",
        "    try:\n",
        "        cv2.grabCut(img_bgr, mask, rect, bgd, fgd, 3, mode=cv2.GC_INIT_WITH_RECT)\n",
        "        fg = (mask == cv2.GC_FGD) | (mask == cv2.GC_PR_FGD)\n",
        "    except Exception:\n",
        "        fg = np.ones((h, w), bool)\n",
        "    return fg\n",
        "\n",
        "def _random_bg_np(shape, pastel_bg: bool):\n",
        "    if not pastel_bg:\n",
        "        return np.random.randint(0, 256, shape, dtype=np.uint8)\n",
        "    base = np.random.randint(100, 256, (1, 1, 3), dtype=np.uint8)\n",
        "    noise = np.random.randint(-30, 30, shape, dtype=np.int16)\n",
        "    return np.clip(base + noise, 0, 255).astype(np.uint8)\n",
        "\n",
        "# --- Random BG worker ---\n",
        "def worker_random_bg(args):\n",
        "    \"\"\"args = (fpath_str, label, out_dir_str, target_size, pastel_bg, labeled_flag)\"\"\"\n",
        "    _init_opencv_worker()\n",
        "    fpath_str, lbl, out_dir_str, target_size, pastel_bg, labeled = args\n",
        "    f = Path(fpath_str)\n",
        "    out_dir = Path(out_dir_str)\n",
        "    img_bgr = cv2.imread(str(f))\n",
        "    if img_bgr is None:\n",
        "        return None\n",
        "    img_bgr = cv2.resize(img_bgr, target_size)\n",
        "    mask = _grabcut_mask_np(img_bgr)\n",
        "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
        "    bg = _random_bg_np(img_rgb.shape, pastel_bg)\n",
        "    comp = np.where(mask[..., None], img_rgb, bg).astype(np.uint8)\n",
        "\n",
        "    if labeled:\n",
        "        dst = out_dir / lbl / f.name\n",
        "        if dst.exists():\n",
        "            dst = out_dir / lbl / f\"{f.stem}_rb_{uuid.uuid4().hex[:6]}{f.suffix}\"\n",
        "    else:\n",
        "        dst = out_dir / f.name\n",
        "\n",
        "    cv2.imwrite(str(dst), cv2.cvtColor(comp, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # Occasionally return a sample path for preview (opened in main proc)\n",
        "    if random.random() < 0.02:\n",
        "        return (str(dst), lbl or f.name)\n",
        "    return None\n",
        "\n",
        "# --- File copy worker (I/O bound, used by combine) ---\n",
        "def worker_copy_file(args):\n",
        "    \"\"\"args = (src_path_str, dst_path_str, label_for_preview)\"\"\"\n",
        "    src_path_str, dst_path_str, lbl = args\n",
        "    src_p, dst_p = Path(src_path_str), Path(dst_path_str)\n",
        "    dst_p.parent.mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        import shutil\n",
        "        shutil.copy2(src_p, dst_p)\n",
        "    except Exception:\n",
        "        return None\n",
        "    if random.random() < 0.02:\n",
        "        return (str(dst_p), lbl)\n",
        "    return None\n",
        "\n",
        "# --- Augmentation worker ---\n",
        "def worker_augment_one(args):\n",
        "    \"\"\"args = (src_path_str, lbl, out_dir_str, target_size, pastel_bg_unused, labeled_flag, aug_conf_dict)\"\"\"\n",
        "    _init_opencv_worker()\n",
        "    src_path_str, lbl, out_dir_str, target_size, _pastel, labeled, aug_conf = args\n",
        "    out_dir = Path(out_dir_str)\n",
        "    try:\n",
        "        # Rebuild the torchvision augment pipeline inside the worker (it’s picklable but safer to rebuild from config)\n",
        "        from torchvision import transforms\n",
        "        from PIL import Image\n",
        "        import torch\n",
        "\n",
        "        class AddGaussianNoise:\n",
        "            def __init__(self, mean=0.0, std=0.015):\n",
        "                self.mean, self.std = mean, std\n",
        "            def __call__(self, img_tensor):\n",
        "                noise = torch.randn_like(img_tensor) * self.std + self.mean\n",
        "                return torch.clamp(img_tensor + noise, 0.0, 1.0)\n",
        "\n",
        "        aug = transforms.Compose([\n",
        "            transforms.Resize(target_size, interpolation=Image.BILINEAR),\n",
        "            transforms.RandomApply([transforms.ColorJitter(\n",
        "                brightness=0.25, contrast=0.25, saturation=0.15, hue=0.02)], p=0.8),\n",
        "            transforms.RandomAffine(degrees=10, translate=(0.05, 0.05),\n",
        "                                    scale=(0.95, 1.05), shear=(-5, 5)),\n",
        "            transforms.RandomPerspective(distortion_scale=0.25, p=0.4),\n",
        "            transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 1.5)),\n",
        "            transforms.ToTensor(),\n",
        "            AddGaussianNoise(mean=0.0, std=0.015),\n",
        "            transforms.ToPILImage()\n",
        "        ])\n",
        "\n",
        "        base = Image.open(src_path_str).convert(\"RGB\")\n",
        "        aug_img = aug(base)\n",
        "        new_name = f\"{Path(src_path_str).stem}_aug_{uuid.uuid4().hex[:8]}.jpg\"\n",
        "        dst = (out_dir / lbl / new_name) if labeled else (out_dir / new_name)\n",
        "        aug_img.save(dst, quality=95)\n",
        "\n",
        "        if random.random() < 0.02:\n",
        "            return (str(dst), lbl or Path(src_path_str).stem)\n",
        "    except Exception:\n",
        "        return None\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ProcessPoolExecutor\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "def make_random_bg_dataset(self, in_dir, out_dir=None):\n",
        "    src = Path(in_dir).resolve()\n",
        "    out = Path(out_dir or f\"{in_dir}_with_random_rgb_values\").resolve()\n",
        "    self._safe_mkdir(out)\n",
        "\n",
        "    label_dirs = [d for d in src.iterdir() if d.is_dir()]\n",
        "    labeled = bool(label_dirs)\n",
        "    if labeled:\n",
        "        for d in label_dirs:\n",
        "            self._safe_mkdir(out / d.name)\n",
        "\n",
        "    # Build jobs list (strings only)\n",
        "    if labeled:\n",
        "        jobs = [(str(f), d.name, str(out), self.target_size, self.pastel_bg, True)\n",
        "                for d in label_dirs for f in d.iterdir() if self._is_image(f)]\n",
        "    else:\n",
        "        jobs = [(str(f), \"\", str(out), self.target_size, self.pastel_bg, False)\n",
        "                for f in src.iterdir() if self._is_image(f)]\n",
        "\n",
        "    previews = []\n",
        "    with ProcessPoolExecutor(max_workers=self.n_workers, initializer=_init_opencv_worker) as ex:\n",
        "        for r in tqdm(ex.map(worker_random_bg, jobs), total=len(jobs), desc=f\"RandomBG {src.name}\"):\n",
        "            if r and len(previews) < 12:\n",
        "                previews.append(r)\n",
        "\n",
        "    # open a few preview paths now\n",
        "    samples = []\n",
        "    for pth, lbl in previews[:9]:\n",
        "        try:\n",
        "            samples.append((Image.open(pth).convert(\"RGB\"), lbl))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    print(f\"✅ Random background dataset saved: {out}\")\n",
        "    self._show_grid(samples, f\"Random RGB Backgrounds: {src.name}\")\n",
        "    return str(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "\n",
        "def combine_datasets(self, src_dirs, combined_dir):\n",
        "    out = Path(combined_dir).resolve()\n",
        "    self._safe_mkdir(out)\n",
        "\n",
        "    label_set = set()\n",
        "    for s in src_dirs:\n",
        "        for d in os.listdir(s):\n",
        "            if (Path(s) / d).is_dir():\n",
        "                label_set.add(d)\n",
        "    for lbl in sorted(label_set):\n",
        "        self._safe_mkdir(out / lbl)\n",
        "\n",
        "    tasks = []\n",
        "    for src in src_dirs:\n",
        "        base = Path(src).name\n",
        "        labeled_subdirs = [d for d in Path(src).iterdir() if d.is_dir()]\n",
        "        if labeled_subdirs:\n",
        "            for d in labeled_subdirs:\n",
        "                for f in d.iterdir():\n",
        "                    if not self._is_image(f): continue\n",
        "                    dst = out / d.name / f.name\n",
        "                    if dst.exists(): dst = out / d.name / f\"{base}_{f.name}\"\n",
        "                    tasks.append((str(f), str(dst), d.name))\n",
        "        else:\n",
        "            for f in [x for x in Path(src).iterdir() if self._is_image(x)]:\n",
        "                dst = out / f.name\n",
        "                if dst.exists(): dst = out / f\"{base}_{f.name}\"\n",
        "                tasks.append((str(f), str(dst), \"\"))\n",
        "\n",
        "    previews = []\n",
        "    with ThreadPoolExecutor(max_workers=min(64, self.n_workers * 4)) as ex:\n",
        "        for r in tqdm(ex.map(worker_copy_file, tasks), total=len(tasks), desc=\"Combining\"):\n",
        "            if r and len(previews) < 12:\n",
        "                previews.append(r)\n",
        "\n",
        "    samples = []\n",
        "    for pth, lbl in previews[:9]:\n",
        "        try:\n",
        "            samples.append((Image.open(pth).convert(\"RGB\"), lbl))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    print(f\"✅ Combined dataset saved: {out}\")\n",
        "    self._show_grid(samples, \"Combined Dataset Preview\")\n",
        "    return str(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import math\n",
        "\n",
        "def augment_20_percent(self, in_dir, out_dir=None, include_originals=True):\n",
        "    src = Path(in_dir).resolve()\n",
        "    out = Path(out_dir or f\"{in_dir}_augmented\").resolve()\n",
        "    self._safe_mkdir(out)\n",
        "\n",
        "    labeled_dirs = [d for d in src.iterdir() if d.is_dir()]\n",
        "    labeled = bool(labeled_dirs)\n",
        "    if labeled:\n",
        "        for d in labeled_dirs: self._safe_mkdir(out / d.name)\n",
        "\n",
        "    files = ([(d.name, f) for d in labeled_dirs for f in d.iterdir() if self._is_image(f)]\n",
        "             if labeled else [(\"\", f) for f in src.iterdir() if self._is_image(f)])\n",
        "    base_total = len(files)\n",
        "    print(f\"Found {base_total} images in {src.name}\")\n",
        "\n",
        "    if include_originals:\n",
        "        def _copy_orig(item):\n",
        "            lbl, f = item\n",
        "            try:\n",
        "                img = Image.open(f).convert(\"RGB\").resize(self.target_size, Image.BILINEAR)\n",
        "                dst = (out / lbl / f.name) if labeled else (out / f.name)\n",
        "                if dst.exists():\n",
        "                    dst = dst.with_name(f\"{dst.stem}_orig_{uuid.uuid4().hex[:6]}{dst.suffix}\")\n",
        "                img.save(dst, quality=95)\n",
        "            except Exception:\n",
        "                pass\n",
        "            return None\n",
        "        with ThreadPoolExecutor(max_workers=min(64, self.n_workers * 4)) as ex:\n",
        "            list(tqdm(ex.map(_copy_orig, files), total=len(files), desc=\"Copying originals\"))\n",
        "\n",
        "    to_add = int(math.ceil(base_total * 0.20))\n",
        "    print(f\"Augmenting +{to_add} images (~20%)...\")\n",
        "\n",
        "    plan = []\n",
        "    if labeled:\n",
        "        per = {d.name: [f for f in d.iterdir() if self._is_image(f)] for d in labeled_dirs}\n",
        "        target = {k: max(0, int(math.ceil(len(v) * 0.20))) for k, v in per.items()}\n",
        "        drift = to_add - sum(target.values())\n",
        "        keys = list(target.keys()); i = 0\n",
        "        while drift != 0 and keys:\n",
        "            target[keys[i % len(keys)]] += 1 if drift > 0 else -1\n",
        "            drift += -1 if drift > 0 else 1\n",
        "            i += 1\n",
        "        for lbl, lst in per.items():\n",
        "            for k in range(target[lbl]):\n",
        "                plan.append((str(lst[k % len(lst)]), lbl, str(out), self.target_size, False, True, {}))\n",
        "    else:\n",
        "        lst = [str(f) for _, f in files]\n",
        "        for k in range(to_add):\n",
        "            plan.append((lst[k % len(lst)], \"\", str(out), self.target_size, False, False, {}))\n",
        "\n",
        "    previews = []\n",
        "    with ProcessPoolExecutor(max_workers=self.n_workers, initializer=_init_opencv_worker) as ex:\n",
        "        for r in tqdm(ex.map(worker_augment_one, plan), total=len(plan), desc=\"Augmenting\"):\n",
        "            if r and len(previews) < 12:\n",
        "                previews.append(r)\n",
        "\n",
        "    samples = []\n",
        "    for pth, lbl in previews[:9]:\n",
        "        try:\n",
        "            samples.append((Image.open(pth).convert(\"RGB\"), lbl))\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    print(f\"✅ Augmentation complete: {out} | Copied: {base_total if include_originals else 0}, Augmented: {len(plan)}\")\n",
        "    self._show_grid(samples, \"Augmented (+20%) Preview\")\n",
        "    return str(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import multiprocessing as mp\n",
        "try:\n",
        "    mp.set_start_method(\"fork\")  # or \"spawn\" if you're on Windows\n",
        "except RuntimeError:\n",
        "    pass  # already set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "niJxG1EfPMEC",
        "outputId": "ed04232b-60e1-4a48-d758-275f26c853e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "RandomBG asl_alphabet_train:   0%|          | 0/87000 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "Can't pickle local object 'ASLPipeline.make_random_bg_dataset.<locals>._do_one'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31m_RemoteTraceback\u001b[39m                          Traceback (most recent call last)",
            "\u001b[31m_RemoteTraceback\u001b[39m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 264, in _feed\n    obj = _ForkingPickler.dumps(obj)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.12/multiprocessing/reduction.py\", line 51, in dumps\n    cls(buf, protocol).dump(obj)\nAttributeError: Can't pickle local object 'ASLPipeline.make_random_bg_dataset.<locals>._do_one'\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      1\u001b[39m train_dir = \u001b[33m\"\u001b[39m\u001b[33m/home/mich02/Desktop/Disability_project_Vietnam/ai4li_VSL/asl_alphabet/asl_alphabet_train\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m pipe = ASLPipeline(\n\u001b[32m      4\u001b[39m     target_size=(\u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m),\n\u001b[32m      5\u001b[39m     pastel_bg=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m      6\u001b[39m     n_workers= \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, os.cpu_count()-\u001b[32m2\u001b[39m),  \u001b[38;5;66;03m# keep 1–2 cores free\u001b[39;00m\n\u001b[32m      7\u001b[39m     backend=\u001b[33m\"\u001b[39m\u001b[33mprocess\u001b[39m\u001b[33m\"\u001b[39m                     \u001b[38;5;66;03m# best for GrabCut + PIL ops\u001b[39;00m\n\u001b[32m      8\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train_rand = \u001b[43mpipe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_random_bg_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m train_comb = pipe.combine_datasets([train_dir, train_rand], \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_combined\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m train_aug  = pipe.augment_20_percent(train_comb)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 144\u001b[39m, in \u001b[36mASLPipeline.make_random_bg_dataset\u001b[39m\u001b[34m(self, in_dir, out_dir)\u001b[39m\n\u001b[32m    142\u001b[39m previews = []\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Executor(max_workers=\u001b[38;5;28mself\u001b[39m.n_workers) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_do_one\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mjobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRandomBG \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msrc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpreviews\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m<\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m12\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpreviews\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Disability_project_Vietnam/ai4li_VSL/ai4li_vsl_env/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/process.py:642\u001b[39m, in \u001b[36m_chain_from_iterable_of_lists\u001b[39m\u001b[34m(iterable)\u001b[39m\n\u001b[32m    636\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[32m    637\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    638\u001b[39m \u001b[33;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[32m    639\u001b[39m \u001b[33;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[32m    640\u001b[39m \u001b[33;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[32m    641\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    643\u001b[39m \u001b[43m        \u001b[49m\u001b[43melement\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mwhile\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melement\u001b[49m\u001b[43m:\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/queues.py:264\u001b[39m, in \u001b[36mQueue._feed\u001b[39m\u001b[34m(buffer, notempty, send_bytes, writelock, reader_close, writer_close, ignore_epipe, onerror, queue_sem)\u001b[39m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    263\u001b[39m \u001b[38;5;66;03m# serialize the data before acquiring the lock\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m obj = \u001b[43m_ForkingPickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wacquire \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    266\u001b[39m     send_bytes(obj)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/multiprocessing/reduction.py:51\u001b[39m, in \u001b[36mForkingPickler.dumps\u001b[39m\u001b[34m(cls, obj, protocol)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     50\u001b[39m     buf = io.BytesIO()\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m buf.getbuffer()\n",
            "\u001b[31mAttributeError\u001b[39m: Can't pickle local object 'ASLPipeline.make_random_bg_dataset.<locals>._do_one'"
          ]
        }
      ],
      "source": [
        "train_dir = \"/home/mich02/Desktop/Disability_project_Vietnam/ai4li_VSL/asl_alphabet/asl_alphabet_train\"\n",
        "\n",
        "pipe = ASLPipeline(\n",
        "    target_size=(224, 224),\n",
        "    pastel_bg=False,\n",
        "    n_workers= max(1, os.cpu_count()-2),  # keep 1–2 cores free\n",
        "    backend=\"process\"                     # best for GrabCut + PIL ops\n",
        ")\n",
        "\n",
        "train_rand = pipe.make_random_bg_dataset(train_dir)\n",
        "train_comb = pipe.combine_datasets([train_dir, train_rand], f\"{train_dir}_combined\")\n",
        "train_aug  = pipe.augment_20_percent(train_comb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO5VQBLQ_7yM"
      },
      "source": [
        "# Vision Transfomer\n",
        "\n",
        "Transformer for CV\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=19ZbTiRydCvsWg5dZXVJsuY2D6y7N3HFg)\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=15ZDgQ2fBMkReXxBwvX3eEa4wAsD407U7)\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1WlKq6shzl4JjpQ7JZR0cW0B7hjJOcQUp)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lpNiUUaUgLP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ai4li_vsl_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
